# -*- coding: utf-8 -*-
"""Fake Review Detection Using NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NU5hAar9cpbC-7UyZSKT5mBCM7IHm1Ab

#Imports and Settings
"""

# STEP 0: Setup
!pip -q install wordcloud nltk

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import matplotlib as mpl

from wordcloud import WordCloud

import re
import string

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer

# Plot settings (nice looking defaults)
plt.rcParams["figure.figsize"] = (10, 5)
plt.rcParams["axes.grid"] = True
plt.rcParams["font.size"] = 12

print("Setup complete.")

# STEP 1: Load data + quick peek

file_path = "/content/fake reviews dataset.csv"  # change if your file name is different
df = pd.read_csv(file_path)

print("Shape (rows, cols):", df.shape)
print("\nColumns:", df.columns.tolist())

print("\nFirst 5 rows:")
display(df.head())

print("\nRandom 5 samples:")
display(df.sample(5, random_state=42))

# Keep only relevant columns in a standard order (safe even if extra columns exist)
needed_cols = ["text_", "label", "rating", "category"]
missing = [c for c in needed_cols if c not in df.columns]
if missing:
    print("\nWARNING: Missing expected columns:", missing)
else:
    df = df[needed_cols].copy()
    print("\nUsing columns:", needed_cols)

# STEP 2: Data quality checks + label distribution

print("Missing values per column:")
print(df.isna().sum())

print("\nDuplicate rows:", df.duplicated().sum())

# Label distribution
label_counts = df["label"].value_counts()
print("\nLabel counts:")
print(label_counts)

# Plot label distribution
plt.figure(figsize=(6,4))
label_counts.plot(kind="bar", color=["#4CAF50", "#FF7043"])
plt.title("Label Distribution (CG vs OR)")
plt.xlabel("Label")
plt.ylabel("Number of Reviews")
plt.xticks(rotation=0)
plt.show()

# Percentage distribution
label_pct = (label_counts / len(df)) * 100
print("\nLabel percentage distribution:")
print(label_pct.round(2))

# STEP 3: Text cleaning (without lemmatization/stemming yet)

# Remove duplicates first
df = df.drop_duplicates().reset_index(drop=True)
print("Shape after removing duplicates:", df.shape)

# Keep original text
df["text_raw"] = df["text_"]

def clean_text_basic(text):
    text = text.lower()
    text = re.sub(r"http\S+|www\S+", "", text)        # remove URLs
    text = re.sub(r"<.*?>", "", text)                 # remove HTML tags
    text = re.sub(r"[^a-z\s]", " ", text)             # remove punctuation & numbers
    text = re.sub(r"\s+", " ", text).strip()          # normalize spaces
    return text

df["text_clean"] = df["text_raw"].apply(clean_text_basic)

print("\nBefore vs After cleaning (5 samples):")
sample_df = df[["text_raw", "text_clean"]].sample(5, random_state=42)
display(sample_df)

# STEP 4.1: Text length features

df["word_count"] = df["text_clean"].apply(lambda x: len(x.split()))
df["char_count"] = df["text_clean"].apply(len)

df[["label", "word_count", "char_count"]].describe()

# STEP 4.2: Word count distribution by label

plt.figure(figsize=(10,5))

for label, color in zip(["CG", "OR"], ["#1E88E5", "#D81B60"]):
    subset = df[df["label"] == label]
    plt.hist(
        subset["word_count"],
        bins=50,
        alpha=0.6,
        label=label,
        color=color
    )

plt.title("Word Count Distribution by Label")
plt.xlabel("Number of Words")
plt.ylabel("Frequency")
plt.legend()
plt.xlim(0, 300)  # limit extreme outliers for readability
plt.show()

# STEP 4.3: Character count distribution by label

plt.figure(figsize=(10,5))

for label, color in zip(["CG", "OR"], ["#43A047", "#FB8C00"]):
    subset = df[df["label"] == label]
    plt.hist(
        subset["char_count"],
        bins=50,
        alpha=0.6,
        label=label,
        color=color
    )

plt.title("Character Count Distribution by Label")
plt.xlabel("Number of Characters")
plt.ylabel("Frequency")
plt.legend()
plt.xlim(0, 2000)
plt.show()

# STEP 4.4: Rating distribution by label

rating_counts = (
    df.groupby(["label", "rating"])
      .size()
      .reset_index(name="count")
)

plt.figure(figsize=(8,5))

for label, color in zip(["CG", "OR"], ["#8E24AA", "#039BE5"]):
    subset = rating_counts[rating_counts["label"] == label]
    plt.plot(
        subset["rating"],
        subset["count"],
        marker="o",
        label=label,
        color=color
    )

plt.title("Rating Distribution by Label")
plt.xlabel("Rating")
plt.ylabel("Number of Reviews")
plt.legend()
plt.grid(True)
plt.show()

# STEP 4.5: Top categories by label

top_categories = (
    df.groupby(["label", "category"])
      .size()
      .reset_index(name="count")
)

# Get top 10 categories per label
top_cg = top_categories[top_categories["label"] == "CG"].nlargest(10, "count")
top_or = top_categories[top_categories["label"] == "OR"].nlargest(10, "count")

plt.figure(figsize=(14,5))

plt.subplot(1,2,1)
plt.barh(top_cg["category"], top_cg["count"], color="#E53935")
plt.title("Top Categories (CG)")
plt.xlabel("Count")

plt.subplot(1,2,2)
plt.barh(top_or["category"], top_or["count"], color="#1E88E5")
plt.title("Top Categories (OR)")
plt.xlabel("Count")

plt.tight_layout()
plt.show()

# STEP 5.1: Combine text by label

cg_text = " ".join(df[df["label"] == "CG"]["text_clean"])
or_text = " ".join(df[df["label"] == "OR"]["text_clean"])

print("CG text length:", len(cg_text))
print("OR text length:", len(or_text))

# STEP 5.2: Word cloud for CG reviews

wc_cg = WordCloud(
    width=800,
    height=400,
    background_color="white",
    colormap="Reds",
    max_words=200
).generate(cg_text)

plt.figure(figsize=(12,6))
plt.imshow(wc_cg, interpolation="bilinear")
plt.axis("off")
plt.title("Word Cloud – CG Reviews", fontsize=16)
plt.show()

# STEP 5.3: Word cloud for OR reviews

wc_or = WordCloud(
    width=800,
    height=400,
    background_color="white",
    colormap="Blues",
    max_words=200
).generate(or_text)

plt.figure(figsize=(12,6))
plt.imshow(wc_or, interpolation="bilinear")
plt.axis("off")
plt.title("Word Cloud – OR Reviews", fontsize=16)
plt.show()

# STEP 6.1: NLP tools setup

stop_words = set(stopwords.words("english"))

lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

print("Stopwords count:", len(stop_words))
print("Example stopwords:", list(stop_words)[:10])

# FIX: download missing tokenizer resource
import nltk
nltk.download("punkt_tab")
print("punkt_tab downloaded successfully")

# STEP 6.2: Tokenization + stopword removal

def tokenize_and_remove_stopwords(text):
    tokens = word_tokenize(text)
    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]
    return tokens

df["tokens"] = df["text_clean"].apply(tokenize_and_remove_stopwords)

df[["text_clean", "tokens"]].sample(5, random_state=42)

# STEP 6.3: Lemmatization

def lemmatize_tokens(tokens):
    return [lemmatizer.lemmatize(token) for token in tokens]

df["tokens_lemma"] = df["tokens"].apply(lemmatize_tokens)

df[["tokens", "tokens_lemma"]].sample(5, random_state=42)

# STEP 6.4: Stemming

def stem_tokens(tokens):
    return [stemmer.stem(token) for token in tokens]

df["tokens_stem"] = df["tokens"].apply(stem_tokens)

df[["tokens", "tokens_stem"]].sample(5, random_state=42)

# STEP 6.5: Rebuild processed text strings

df["text_lemma"] = df["tokens_lemma"].apply(lambda x: " ".join(x))
df["text_stem"] = df["tokens_stem"].apply(lambda x: " ".join(x))

df[["text_clean", "text_lemma", "text_stem"]].sample(5, random_state=42)

# STEP 7.1: Top words by label (after lemmatization)

from collections import Counter

def get_top_words(df_subset, col, top_n=20):
    all_tokens = " ".join(df_subset[col]).split()
    return Counter(all_tokens).most_common(top_n)

top_cg_words = get_top_words(df[df["label"]=="CG"], "text_lemma", 20)
top_or_words = get_top_words(df[df["label"]=="OR"], "text_lemma", 20)

cg_words, cg_counts = zip(*top_cg_words)
or_words, or_counts = zip(*top_or_words)

plt.figure(figsize=(14,5))

plt.subplot(1,2,1)
plt.barh(cg_words[::-1], cg_counts[::-1], color="#E53935")
plt.title("Top 20 Words - CG (Lemmatized)")
plt.xlabel("Frequency")

plt.subplot(1,2,2)
plt.barh(or_words[::-1], or_counts[::-1], color="#1E88E5")
plt.title("Top 20 Words - OR (Lemmatized)")
plt.xlabel("Frequency")

plt.tight_layout()
plt.show()

# STEP 7.2: Vocabulary size comparison

def vocab_size(series):
    words = " ".join(series).split()
    return len(set(words))

v_clean = vocab_size(df["text_clean"])
v_lemma = vocab_size(df["text_lemma"])
v_stem  = vocab_size(df["text_stem"])

print("Vocabulary Size")
print("text_clean:", v_clean)
print("text_lemma:", v_lemma)
print("text_stem :", v_stem)

plt.figure(figsize=(7,4))
plt.bar(["clean", "lemma", "stem"], [v_clean, v_lemma, v_stem], color=["#43A047","#1E88E5","#FB8C00"])
plt.title("Vocabulary Size Comparison")
plt.ylabel("Unique words")
plt.show()

# STEP 7.3: TF-IDF feature inspection

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(
    stop_words="english",
    ngram_range=(1,2),
    max_features=30000,
    min_df=2
)

X_tfidf = tfidf.fit_transform(df["text_lemma"])
print("TF-IDF matrix shape:", X_tfidf.shape)

feature_names = tfidf.get_feature_names_out()
print("Number of features:", len(feature_names))
print("Sample features:", feature_names[:30])

# STEP 8.1: Train-test split

from sklearn.model_selection import train_test_split

X_clean = df["text_clean"]
X_lemma = df["text_lemma"]
y = df["label"]

Xc_train, Xc_test, y_train, y_test = train_test_split(
    X_clean, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

Xl_train, Xl_test, _, _ = train_test_split(
    X_lemma, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

print("Train size:", Xc_train.shape[0])
print("Test size :", Xc_test.shape[0])

# STEP 8.2: Logistic Regression on clean text

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

pipe_clean = Pipeline([
    ("tfidf", TfidfVectorizer(
        stop_words="english",
        ngram_range=(1,2),
        max_features=30000,
        min_df=2
    )),
    ("clf", LogisticRegression(max_iter=3000))
])

pipe_clean.fit(Xc_train, y_train)

y_pred_clean = pipe_clean.predict(Xc_test)

print("Logistic Regression (Clean Text)")
print(classification_report(y_test, y_pred_clean))

# STEP 8.3: Logistic Regression on lemmatized text

pipe_lemma = Pipeline([
    ("tfidf", TfidfVectorizer(
        stop_words="english",
        ngram_range=(1,2),
        max_features=30000,
        min_df=2
    )),
    ("clf", LogisticRegression(max_iter=3000))
])

pipe_lemma.fit(Xl_train, y_train)

y_pred_lemma = pipe_lemma.predict(Xl_test)

print("Logistic Regression (Lemmatized Text)")
print(classification_report(y_test, y_pred_lemma))

from sklearn.metrics import accuracy_score

results = pd.DataFrame({
    "Text Version": ["Clean", "Lemmatized"],
    "Accuracy": [
        accuracy_score(y_test, y_pred_clean),
        accuracy_score(y_test, y_pred_lemma)
    ]
})

results

# STEP 9.1: Linear SVM on clean text

from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report

svm_clean = Pipeline([
    ("tfidf", TfidfVectorizer(
        stop_words="english",
        ngram_range=(1,2),
        max_features=30000,
        min_df=2
    )),
    ("clf", LinearSVC())
])

svm_clean.fit(Xc_train, y_train)

y_pred_svm = svm_clean.predict(Xc_test)

print("Linear SVM (Clean Text)")
print(classification_report(y_test, y_pred_svm))

from sklearn.metrics import accuracy_score

comparison = pd.DataFrame({
    "Model": ["Logistic Regression (Clean)", "Linear SVM (Clean)"],
    "Accuracy": [
        accuracy_score(y_test, y_pred_clean),
        accuracy_score(y_test, y_pred_svm)
    ]
})

comparison

# STEP 10.1: Get decision scores from SVM

decision_scores = svm_clean.decision_function(Xc_test)

results = pd.DataFrame({
    "text": Xc_test.values,
    "actual": y_test.values,
    "predicted": y_pred_svm,
    "confidence": np.abs(decision_scores)
})

print("Total test samples:", results.shape[0])
results.head()

# STEP 10.2: Most confident wrong predictions

wrong_preds = results[results["actual"] != results["predicted"]]
wrong_preds_sorted = wrong_preds.sort_values(
    by="confidence", ascending=False
)

wrong_preds_sorted.head(10)

# STEP 10.3: Inspect confusing examples

for i, row in wrong_preds_sorted.head(5).iterrows():
    print("Actual:", row["actual"])
    print("Predicted:", row["predicted"])
    print("Confidence:", round(row["confidence"], 3))
    print("Text:", row["text"][:500])
    print("-" * 80)

# STEP 11.1: Extract SVM feature weights

svm_clf = svm_clean.named_steps["clf"]
tfidf_vec = svm_clean.named_steps["tfidf"]

feature_names = np.array(tfidf_vec.get_feature_names_out())
coefficients = svm_clf.coef_[0]

print("Number of features:", len(feature_names))

print("Class order:", svm_clf.classes_)

# STEP 11.3: Top weighted words per class

top_n = 20

top_or_idx = np.argsort(coefficients)[-top_n:][::-1]
top_cg_idx = np.argsort(coefficients)[:top_n]

top_or_words = feature_names[top_or_idx]
top_or_weights = coefficients[top_or_idx]

top_cg_words = feature_names[top_cg_idx]
top_cg_weights = coefficients[top_cg_idx]

print("Top words pushing toward OR:\n")
for w, c in zip(top_or_words, top_or_weights):
    print(f"{w:20s} {c:.3f}")

print("\nTop words pushing toward CG:\n")
for w, c in zip(top_cg_words, top_cg_weights):
    print(f"{w:20s} {c:.3f}")

# STEP 11.4: Visualization of influential words

plt.figure(figsize=(14,5))

plt.subplot(1,2,1)
plt.barh(top_or_words[::-1], top_or_weights[::-1], color="#1E88E5")
plt.title("Top Words Pushing Toward OR")
plt.xlabel("SVM Weight")

plt.subplot(1,2,2)
plt.barh(top_cg_words[::-1], top_cg_weights[::-1], color="#E53935")
plt.title("Top Words Pushing Toward CG")
plt.xlabel("SVM Weight")

plt.tight_layout()
plt.show()

# STEP 12.1: Save trained model

import joblib

model_path = "/content/fake_review_svm_model.joblib"
joblib.dump(svm_clean, model_path)

print("Model saved to:", model_path)

# STEP 12.2: Inference function

def predict_review(text, model=svm_clean):
    text = clean_text_basic(text)
    decision = model.decision_function([text])[0]
    label = model.predict([text])[0]
    confidence = abs(decision)
    return {
        "prediction": label,
        "confidence": round(confidence, 3)
    }

# STEP 12.3: Test predictions

samples = [
    "Absolutely loved this product. Works perfectly and exceeded my expectations.",
    "Bought this for my mother after comparing several options. It has been working well for over 6 months."
]

for s in samples:
    print(s)
    print(predict_review(s))
    print("-" * 60)